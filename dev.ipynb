{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2d349546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: torch.Size([1115394]), Type: torch.int64.\n",
      "Number of train samples is 1003854. Number of validation samples is 111540.\n",
      "Step: 0. Train loss: 4.122864723205566. Validation loss: 4.122955799102783\n",
      "Generated text is \n",
      " \n",
      "SqaVBGzyea.\n",
      "Step: 10. Train loss: 4.019296169281006. Validation loss: 4.034889221191406\n",
      "Generated text is \n",
      " \n",
      "3VbgkLrVVX.\n",
      "Step: 20. Train loss: 3.9486913681030273. Validation loss: 3.9343836307525635\n",
      "Generated text is \n",
      " \n",
      "d?jSr:ta!3.\n",
      "Step: 30. Train loss: 3.845771074295044. Validation loss: 3.8109352588653564\n",
      "Generated text is \n",
      " \n",
      "e&as'Ca,wg.\n",
      "Step: 40. Train loss: 3.6778366565704346. Validation loss: 3.726238489151001\n",
      "Generated text is \n",
      " \n",
      "ozRd\n",
      "I$n: .\n",
      "Step: 50. Train loss: 3.540225028991699. Validation loss: 3.664419412612915\n",
      "Generated text is \n",
      " \n",
      "kaZUri OOo.\n",
      "Step: 60. Train loss: 3.469433069229126. Validation loss: 3.4516074657440186\n",
      "Generated text is \n",
      " \n",
      "a  AaXual .\n",
      "Step: 70. Train loss: 3.41318416595459. Validation loss: 3.416707754135132\n",
      "Generated text is \n",
      " \n",
      " Y& SlvgaG.\n",
      "Step: 80. Train loss: 3.327633857727051. Validation loss: 3.4585437774658203\n",
      "Generated text is \n",
      " \n",
      "e-di !ti  .\n",
      "Step: 90. Train loss: 3.297860860824585. Validation loss: 3.415867567062378\n",
      "Generated text is \n",
      " \n",
      " Cnnar trm.\n",
      "Step: 100. Train loss: 3.3431012630462646. Validation loss: 3.3529093265533447\n",
      "Generated text is \n",
      " \n",
      "a .laRe li.\n",
      "Step: 110. Train loss: 3.326279878616333. Validation loss: 3.2965362071990967\n",
      "Generated text is \n",
      " \n",
      "\n",
      "eo tYh,hm.\n",
      "Step: 120. Train loss: 3.16686749458313. Validation loss: 3.362433433532715\n",
      "Generated text is \n",
      " \n",
      "etiiaOitok.\n",
      "Step: 130. Train loss: 3.1684253215789795. Validation loss: 3.2571868896484375\n",
      "Generated text is \n",
      " \n",
      "\n",
      "s 'ehsS  .\n",
      "Step: 140. Train loss: 3.1439144611358643. Validation loss: 3.273145914077759\n",
      "Generated text is \n",
      " \n",
      "y\n",
      "mrAt se:.\n",
      "Step: 150. Train loss: 3.1800193786621094. Validation loss: 3.2618141174316406\n",
      "Generated text is \n",
      " \n",
      " uuh u \n",
      "\n",
      "t.\n",
      "Step: 160. Train loss: 3.1494739055633545. Validation loss: 3.202566146850586\n",
      "Generated text is \n",
      " \n",
      "hlrionhaoh.\n",
      "Step: 170. Train loss: 3.1828362941741943. Validation loss: 3.250688314437866\n",
      "Generated text is \n",
      " \n",
      "\n",
      ".e, aol,,.\n",
      "Step: 180. Train loss: 3.0148918628692627. Validation loss: 3.2280333042144775\n",
      "Generated text is \n",
      " \n",
      "t  tod fh..\n",
      "Step: 190. Train loss: 3.0829553604125977. Validation loss: 3.177403211593628\n",
      "Generated text is \n",
      " \n",
      "r mee: W T.\n",
      "Step: 200. Train loss: 3.06595778465271. Validation loss: 3.180328607559204\n",
      "Generated text is \n",
      " \n",
      "lorsh\n",
      "vg\n",
      "o.\n",
      "Step: 210. Train loss: 3.053373336791992. Validation loss: 3.213491678237915\n",
      "Generated text is \n",
      " \n",
      "oe\n",
      "Oh \n",
      "te .\n",
      "Step: 220. Train loss: 3.1811866760253906. Validation loss: 3.246736764907837\n",
      "Generated text is \n",
      " \n",
      "rrs: nno.o.\n",
      "Step: 230. Train loss: 3.09584641456604. Validation loss: 3.0978729724884033\n",
      "Generated text is \n",
      " \n",
      "c ,\n",
      "toe,iN.\n",
      "Step: 240. Train loss: 3.0921103954315186. Validation loss: 3.06113600730896\n",
      "Generated text is \n",
      " \n",
      "u  uh o\n",
      "L'.\n",
      "Step: 250. Train loss: 3.0031158924102783. Validation loss: 3.1380319595336914\n",
      "Generated text is \n",
      " \n",
      "leNolab be.\n",
      "Step: 260. Train loss: 3.0976057052612305. Validation loss: 3.126807451248169\n",
      "Generated text is \n",
      " \n",
      "eo megoii .\n",
      "Step: 270. Train loss: 3.1944494247436523. Validation loss: 3.1589479446411133\n",
      "Generated text is \n",
      " \n",
      "iieee\n",
      "Ll l.\n",
      "Step: 280. Train loss: 2.919886350631714. Validation loss: 2.9291515350341797\n",
      "Generated text is \n",
      " \n",
      "ee'Wi n:;e.\n",
      "Step: 290. Train loss: 3.0425329208374023. Validation loss: 3.0012338161468506\n",
      "Generated text is \n",
      " \n",
      "aeoishh r .\n",
      "Step: 300. Train loss: 2.8445746898651123. Validation loss: 3.225104331970215\n",
      "Generated text is \n",
      " \n",
      "aute tc\n",
      "Mt.\n",
      "Step: 310. Train loss: 3.0233688354492188. Validation loss: 2.9573793411254883\n",
      "Generated text is \n",
      " \n",
      "toicrd cfr.\n",
      "Step: 320. Train loss: 2.9634056091308594. Validation loss: 3.0763275623321533\n",
      "Generated text is \n",
      " \n",
      "amoa s uha.\n",
      "Step: 330. Train loss: 3.046130895614624. Validation loss: 3.0505189895629883\n",
      "Generated text is \n",
      " \n",
      "tereoodyrl.\n",
      "Step: 340. Train loss: 2.894418954849243. Validation loss: 2.942279577255249\n",
      "Generated text is \n",
      " \n",
      ";h n H uCi.\n",
      "Step: 350. Train loss: 2.9228436946868896. Validation loss: 3.1024014949798584\n",
      "Generated text is \n",
      " \n",
      "eaab Ie ms.\n",
      "Step: 360. Train loss: 2.9236342906951904. Validation loss: 3.0039749145507812\n",
      "Generated text is \n",
      " \n",
      "s b,e\n",
      "tehO.\n",
      "Step: 370. Train loss: 2.8781044483184814. Validation loss: 2.9891393184661865\n",
      "Generated text is \n",
      " \n",
      "sule mivVE.\n",
      "Step: 380. Train loss: 2.9986531734466553. Validation loss: 2.972550392150879\n",
      "Generated text is \n",
      " \n",
      "g-d np t\n",
      "\n",
      ".\n",
      "Step: 390. Train loss: 2.9281609058380127. Validation loss: 2.9658148288726807\n",
      "Generated text is \n",
      " \n",
      "m m hh w w.\n",
      "Step: 400. Train loss: 2.894134521484375. Validation loss: 2.8703153133392334\n",
      "Generated text is \n",
      " \n",
      "s wO p tow.\n",
      "Step: 410. Train loss: 2.9478445053100586. Validation loss: 3.0787665843963623\n",
      "Generated text is \n",
      " \n",
      "o N; messl.\n",
      "Step: 420. Train loss: 2.933988571166992. Validation loss: 2.9374608993530273\n",
      "Generated text is \n",
      " \n",
      "Al phanl i.\n",
      "Step: 430. Train loss: 2.784621477127075. Validation loss: 3.0990607738494873\n",
      "Generated text is \n",
      " \n",
      "lnmehl uas.\n",
      "Step: 440. Train loss: 2.989901304244995. Validation loss: 2.858337640762329\n",
      "Generated text is \n",
      " \n",
      "i ar m\n",
      "& y.\n",
      "Step: 450. Train loss: 2.9263789653778076. Validation loss: 2.887152910232544\n",
      "Generated text is \n",
      " \n",
      "uin t'u s .\n",
      "Step: 460. Train loss: 2.792245626449585. Validation loss: 2.9156179428100586\n",
      "Generated text is \n",
      " \n",
      "h f tgadr .\n",
      "Step: 470. Train loss: 2.835458755493164. Validation loss: 2.8554811477661133\n",
      "Generated text is \n",
      " \n",
      "eled R ,hu.\n",
      "Step: 480. Train loss: 2.871354103088379. Validation loss: 2.9471304416656494\n",
      "Generated text is \n",
      " \n",
      "ferR sar e.\n",
      "Step: 490. Train loss: 2.8242692947387695. Validation loss: 2.8443381786346436\n",
      "Generated text is \n",
      " \n",
      "caeNanen:h.\n",
      "Step: 500. Train loss: 2.834031343460083. Validation loss: 2.9450912475585938\n",
      "Generated text is \n",
      " \n",
      "wl l . s m.\n",
      "Step: 510. Train loss: 2.688400983810425. Validation loss: 2.904222249984741\n",
      "Generated text is \n",
      " \n",
      "geegm reth.\n",
      "Step: 520. Train loss: 2.7037744522094727. Validation loss: 2.862074613571167\n",
      "Generated text is \n",
      " \n",
      "o:ad\n",
      "hoq t.\n",
      "Step: 530. Train loss: 2.6827011108398438. Validation loss: 2.7789828777313232\n",
      "Generated text is \n",
      " \n",
      "l S, hothh.\n",
      "Step: 540. Train loss: 2.8416833877563477. Validation loss: 2.75421142578125\n",
      "Generated text is \n",
      " \n",
      "r  f t p s.\n",
      "Step: 550. Train loss: 2.729008436203003. Validation loss: 2.8596038818359375\n",
      "Generated text is \n",
      " \n",
      "o se bley .\n",
      "Step: 560. Train loss: 2.7363014221191406. Validation loss: 2.7880542278289795\n",
      "Generated text is \n",
      " \n",
      "e tung mom.\n",
      "Step: 570. Train loss: 2.9051733016967773. Validation loss: 2.9169485569000244\n",
      "Generated text is \n",
      " \n",
      ", wi cie w.\n",
      "Step: 580. Train loss: 2.6584622859954834. Validation loss: 2.8635337352752686\n",
      "Generated text is \n",
      " \n",
      "a wr es gi.\n",
      "Step: 590. Train loss: 2.801192283630371. Validation loss: 2.8034708499908447\n",
      "Generated text is \n",
      " \n",
      "rim:\n",
      "Id\n",
      "OS.\n",
      "Step: 600. Train loss: 2.7291078567504883. Validation loss: 2.781888723373413\n",
      "Generated text is \n",
      " \n",
      "i gos IT\n",
      "A.\n",
      "Step: 610. Train loss: 2.850839376449585. Validation loss: 2.8177146911621094\n",
      "Generated text is \n",
      " \n",
      "iade te:\n",
      "w.\n",
      "Step: 620. Train loss: 2.757690191268921. Validation loss: 2.7541916370391846\n",
      "Generated text is \n",
      " \n",
      ", Eoren,Ie.\n",
      "Step: 630. Train loss: 2.7793285846710205. Validation loss: 2.9437854290008545\n",
      "Generated text is \n",
      " \n",
      "eoiuoofini.\n",
      "Step: 640. Train loss: 2.7215585708618164. Validation loss: 2.7547385692596436\n",
      "Generated text is \n",
      " \n",
      "genseG in..\n",
      "Step: 650. Train loss: 2.705341339111328. Validation loss: 2.771763563156128\n",
      "Generated text is \n",
      " \n",
      "uo p hi ig.\n",
      "Step: 660. Train loss: 2.8100316524505615. Validation loss: 2.7169151306152344\n",
      "Generated text is \n",
      " \n",
      "otu's teve.\n",
      "Step: 670. Train loss: 2.835437059402466. Validation loss: 2.6878788471221924\n",
      "Generated text is \n",
      " \n",
      "t,art! y l.\n",
      "Step: 680. Train loss: 2.6879565715789795. Validation loss: 2.691077947616577\n",
      "Generated text is \n",
      " \n",
      "ne M dnopo.\n",
      "Step: 690. Train loss: 2.6682796478271484. Validation loss: 2.728097677230835\n",
      "Generated text is \n",
      " \n",
      "yreauovpo .\n",
      "Step: 700. Train loss: 2.705678939819336. Validation loss: 2.7547733783721924\n",
      "Generated text is \n",
      " \n",
      "miteI p lc.\n",
      "Step: 710. Train loss: 2.588989496231079. Validation loss: 2.714463233947754\n",
      "Generated text is \n",
      " \n",
      "was tathip.\n",
      "Step: 720. Train loss: 2.804478406906128. Validation loss: 2.840534210205078\n",
      "Generated text is \n",
      " \n",
      "line oun.h.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 730. Train loss: 2.636291027069092. Validation loss: 2.6672399044036865\n",
      "Generated text is \n",
      " \n",
      "te ganir w.\n",
      "Step: 740. Train loss: 2.8083455562591553. Validation loss: 2.704103469848633\n",
      "Generated text is \n",
      " \n",
      "erthiyar;\n",
      ".\n",
      "Step: 750. Train loss: 2.663700819015503. Validation loss: 2.6706902980804443\n",
      "Generated text is \n",
      " \n",
      "oe wedor,\n",
      ".\n",
      "Step: 760. Train loss: 2.7075910568237305. Validation loss: 2.7369544506073\n",
      "Generated text is \n",
      " \n",
      "rshakochov.\n",
      "Step: 770. Train loss: 2.7195827960968018. Validation loss: 2.6510214805603027\n",
      "Generated text is \n",
      " \n",
      "l la. t ce.\n",
      "Step: 780. Train loss: 2.621515989303589. Validation loss: 2.7820844650268555\n",
      "Generated text is \n",
      " \n",
      "i HDe n-i .\n",
      "Step: 790. Train loss: 2.6632611751556396. Validation loss: 2.766740083694458\n",
      "Generated text is \n",
      " \n",
      "le icoard:.\n",
      "Step: 800. Train loss: 2.7473552227020264. Validation loss: 2.641324996948242\n",
      "Generated text is \n",
      " \n",
      "i flt y hy.\n",
      "Step: 810. Train loss: 2.7233469486236572. Validation loss: 2.7877390384674072\n",
      "Generated text is \n",
      " \n",
      "er g bi ag.\n",
      "Step: 820. Train loss: 2.75483775138855. Validation loss: 2.7602298259735107\n",
      "Generated text is \n",
      " \n",
      "leunan ago.\n",
      "Step: 830. Train loss: 2.6924991607666016. Validation loss: 2.762407064437866\n",
      "Generated text is \n",
      " \n",
      "okir dhidh.\n",
      "Step: 840. Train loss: 2.624354124069214. Validation loss: 2.704834222793579\n",
      "Generated text is \n",
      " \n",
      "aeeNset, s.\n",
      "Step: 850. Train loss: 2.693260908126831. Validation loss: 2.6313889026641846\n",
      "Generated text is \n",
      " \n",
      "oa\n",
      "soNo,\n",
      "w.\n",
      "Step: 860. Train loss: 2.6702566146850586. Validation loss: 2.638568639755249\n",
      "Generated text is \n",
      " \n",
      "acrg yies?.\n",
      "Step: 870. Train loss: 2.6716020107269287. Validation loss: 2.6478350162506104\n",
      "Generated text is \n",
      " \n",
      "tol morrgo.\n",
      "Step: 880. Train loss: 2.6350324153900146. Validation loss: 2.67529296875\n",
      "Generated text is \n",
      " \n",
      "uahyt. enn.\n",
      "Step: 890. Train loss: 2.528559923171997. Validation loss: 2.722386121749878\n",
      "Generated text is \n",
      " \n",
      "kin tor u .\n",
      "Step: 900. Train loss: 2.712327003479004. Validation loss: 2.568958282470703\n",
      "Generated text is \n",
      " \n",
      "aiiqte la .\n",
      "Step: 910. Train loss: 2.6728098392486572. Validation loss: 2.690831422805786\n",
      "Generated text is \n",
      " \n",
      "eete ad tr.\n",
      "Step: 920. Train loss: 2.589672327041626. Validation loss: 2.687727689743042\n",
      "Generated text is \n",
      " \n",
      "r o alhro\n",
      ".\n",
      "Step: 930. Train loss: 2.6593401432037354. Validation loss: 2.6886651515960693\n",
      "Generated text is \n",
      " \n",
      "i apeBr ra.\n",
      "Step: 940. Train loss: 2.5757486820220947. Validation loss: 2.701871156692505\n",
      "Generated text is \n",
      " \n",
      "aoeinicibo.\n",
      "Step: 950. Train loss: 2.5866081714630127. Validation loss: 2.6331140995025635\n",
      "Generated text is \n",
      " \n",
      "lr ae ore .\n",
      "Step: 960. Train loss: 2.6389729976654053. Validation loss: 2.6566617488861084\n",
      "Generated text is \n",
      " \n",
      "uee Haa wi.\n",
      "Step: 970. Train loss: 2.5597195625305176. Validation loss: 2.6242308616638184\n",
      "Generated text is \n",
      " \n",
      "aeer yo s..\n",
      "Step: 980. Train loss: 2.6498947143554688. Validation loss: 2.773721694946289\n",
      "Generated text is \n",
      " \n",
      "ylyic, lIo.\n",
      "Step: 990. Train loss: 2.6228549480438232. Validation loss: 2.623080015182495\n",
      "Generated text is \n",
      " \n",
      "be cery i .\n"
     ]
    }
   ],
   "source": [
    "from collections.abc import Callable\n",
    "import torch, torch.nn as nn\n",
    "torch.manual_seed(123)\n",
    "\n",
    "def read_dataset(filename: str, visualize: bool = False) -> str:\n",
    "    ''' Read a text file and return the contents as a string. \n",
    "    \n",
    "        Args:\n",
    "            filename - Path of the text file to be read.\n",
    "            visualize - Whether to visualize the statistics of the file contents.\n",
    "            \n",
    "        Returns\n",
    "            Content of the file as a string.\n",
    "    '''\n",
    "    with open(filename, 'r') as reader:\n",
    "        data = reader.read()\n",
    "        \n",
    "    if visualize:\n",
    "        print(f'Visualizing dataset at path {filename}.')\n",
    "        print(f'First 100 characters:\\n{data[0:100]}.')\n",
    "        print(f'Length: {len(data)}.')\n",
    "    return data\n",
    "        \n",
    "def create_vocabulary(data: str, visualize: bool = False) -> (list[str], dict[str, int], dict[int, str], Callable[str, list[int]], Callable[list[int], str]):\n",
    "    vocabulary = sorted(list(set(data)))\n",
    "    token_to_index_map = {token:index for (index, token) in enumerate(vocabulary)}\n",
    "    index_to_token_map = {index:token for (index, token) in enumerate(vocabulary)}\n",
    "\n",
    "    if visualize:\n",
    "        print(f'Visualizing vocabulary.')\n",
    "        print(f'Length of vocabulary: {len(vocabulary)}.')\n",
    "        print(f'Vocabulary is {\"\".join(vocabulary)}.')\n",
    "        print(f'Token to index map sorted is {token_to_index_map}')\n",
    "        print(f'Index to token map sorted is {index_to_token_map}')              \n",
    "        \n",
    "    def encoder(input: str) -> (list[int]):\n",
    "        ''' Encodes the input string. \n",
    "            \n",
    "            Args:\n",
    "                input: string of text to be encoded.\n",
    "                \n",
    "            Returns:\n",
    "                List of indices of the tokens in the input string.\n",
    "        '''\n",
    "        return [token_to_index_map[token] for token in input]\n",
    "    \n",
    "    def decoder(input: list[int]) -> str:\n",
    "        ''' Decodes the input token index into text.\n",
    "        \n",
    "            Args:\n",
    "                input: List of indices of tokens in the text to be decoded.\n",
    "                \n",
    "            Returns:\n",
    "                String corresponding to the decoded text.\n",
    "        '''\n",
    "        return ''.join([index_to_token_map[index] for index in input])\n",
    "        \n",
    "    return (vocabulary, token_to_index_map, index_to_token_map, encoder, decoder)\n",
    "\n",
    "def run_tokenizer_example(run: bool = False) -> None:\n",
    "    ''' Run example text using character level tokenizer.'''\n",
    "    if run:\n",
    "        print('Running Tokenizer example.')\n",
    "        input_text = 'Hello, how are you?'\n",
    "        tokenized_text = encoder(input_text)\n",
    "        decoded_text = decoder(tokenized_text)\n",
    "        print(f'{input_text=}, {tokenized_text=}, {decoded_text=}.')\n",
    "        \n",
    "def visualize_batch(x, y, skip_visualization: bool = True):\n",
    "    if not skip_visualization:\n",
    "        for sample in range(x.shape[0]):\n",
    "            for context in range(x.shape[1]):\n",
    "                print(f' Context: {x[sample, :context+1]}. Target: {y[sample, context]}.')\n",
    "\n",
    "def create_batch(split, block_size, batch_size):\n",
    "    data = train_set if split == 'train' else val_set\n",
    "    batch_start_index = torch.randint(0, len(data) - block_size - 1, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in batch_start_index])\n",
    "    y = torch.stack([data[i+1: i+1+block_size] for i in batch_start_index])\n",
    "    return (x,y)\n",
    "            \n",
    "@torch.no_grad()\n",
    "def evaluate_loss(batch_index, model, batch_size, block_size):\n",
    "    model.eval()\n",
    "\n",
    "    (x, y) = create_batch('train', block_size, batch_size)\n",
    "    predictions = model(x)\n",
    "    (B, T, C) = predictions.shape\n",
    "    predictions = predictions.view(B*T, C)\n",
    "    y = y.view(-1)\n",
    "    train_loss = nn.functional.cross_entropy(predictions, y)\n",
    "    \n",
    "    (x, y) = create_batch('val', block_size, batch_size)\n",
    "    predictions = model(x)\n",
    "    (B, T, C) = predictions.shape\n",
    "    predictions = predictions.view(B*T, C)\n",
    "    y = y.view(-1)\n",
    "    val_loss = nn.functional.cross_entropy(predictions, y)\n",
    "    model.train()\n",
    "    \n",
    "    print(f'Step: {batch_index}. Train loss: {train_loss.item()}. Validation loss: {val_loss.item()}')\n",
    "    \n",
    "\n",
    "# Read input file.\n",
    "filename = 'data/tinyshakespeare.txt'\n",
    "data = read_dataset(filename)\n",
    "(vocabulary, token_to_index_map, index_to_token_map, encoder, decoder) = create_vocabulary(data, False)\n",
    "run_tokenizer_example(False)\n",
    "\n",
    "# Tokenize dataset.\n",
    "input_sequence = torch.tensor(encoder(data), dtype=torch.long)\n",
    "#print(f'Tokenized input sequence is {input_sequence}.')\n",
    "print(f'Shape: {input_sequence.shape}, Type: {input_sequence.dtype}.')\n",
    "\n",
    "dataset_split_fraction = 0.9\n",
    "num_train_samples = int(dataset_split_fraction * len(data))\n",
    "train_set = input_sequence[:num_train_samples]\n",
    "val_set = input_sequence[num_train_samples:]\n",
    "print(f'Number of train samples is {len(train_set)}. Number of validation samples is {len(val_set)}.')\n",
    "\n",
    "\n",
    "\n",
    "max_block_size = 24\n",
    "batch_size = 32\n",
    "num_batches = 1000\n",
    "num_decoder_blocks = 10\n",
    "vocabulary_size = len(vocabulary)\n",
    "embedding_dimension = 64\n",
    "num_heads = 8\n",
    "head_dimension = 16\n",
    "\n",
    "\n",
    "#model = BigramLanguageModel(len(vocabulary))\n",
    "model = GPT(num_decoder_blocks, vocabulary_size, embedding_dimension, num_heads, head_dimension, max_block_size)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "for batch_index in range(num_batches):\n",
    "    (x, y) = create_batch('train', max_block_size, batch_size)\n",
    "    visualize_batch(x, y, True)\n",
    "    predictions = model(x)\n",
    "\n",
    "    (B, T, C) = predictions.shape\n",
    "    predictions = predictions.view(B*T, C)\n",
    "    y = y.view(-1)\n",
    "    loss = nn.functional.cross_entropy(predictions, y)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if batch_index % 10 == 0:\n",
    "        evaluate_loss(batch_index, model, batch_size, max_block_size)\n",
    "        generated_tokens = model.generate(torch.zeros((1, 1), dtype = torch.long), 10).tolist()[0]\n",
    "        generated_text = decoder(generated_tokens)\n",
    "        print(f'Generated text is \\n {generated_text}.')\n",
    "\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d4b65e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "03a8fb2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[0.2745,   -inf,   -inf],\n",
      "        [0.8573, 0.8993,   -inf],\n",
      "        [0.9268, 0.7388, 0.7179]])\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.4895, 0.5105, 0.0000],\n",
      "        [0.3788, 0.3138, 0.3074]])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5ce8dd89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1115,  0.1204, -0.3696, -0.2404],\n",
      "        [-1.1969,  0.2093, -0.9724, -0.7550],\n",
      "        [ 0.3239, -0.1085,  0.2103, -0.3908]])\n",
      "tensor([-0.6012, -2.7151,  0.0349])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d4d486de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT(\n",
      "  (token_embedding_layer): Embedding(65, 64)\n",
      "  (positional_encoding_layer): Embedding(24, 64)\n",
      "  (transformer_decoders): ModuleList(\n",
      "    (0-9): 10 x TransformerDecoderBlock(\n",
      "      (attention_layer): MultiHeadMaskedAttention(\n",
      "        (Wq): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (Wk): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (Wv): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (head_merge_layer): Linear(in_features=64, out_features=64, bias=True)\n",
      "      )\n",
      "      (mlp_layer): MLP(\n",
      "        (layer): Sequential(\n",
      "          (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (head_layer): Linear(in_features=64, out_features=65, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656a987c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
