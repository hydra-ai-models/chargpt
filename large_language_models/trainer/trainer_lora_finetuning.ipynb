{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea8102a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in the model is 27000.\n",
      " Batch index: 0, train loss: 3.5347208976745605, val_loss: 3.6001789569854736, generated text\n",
      " \n",
      "eyne o  ee\n",
      " Batch index: 10, train loss: 3.428671360015869, val_loss: 3.48527455329895, generated text\n",
      " \n",
      "roid  ,eda\n",
      " Batch index: 20, train loss: 3.342963695526123, val_loss: 3.4461426734924316, generated text\n",
      " \n",
      "Ou l  e - \n",
      " Batch index: 30, train loss: 3.3243308067321777, val_loss: 3.3382644653320312, generated text\n",
      " \n",
      "u  e:r.dna\n",
      " Batch index: 40, train loss: 3.2772433757781982, val_loss: 3.3032562732696533, generated text\n",
      " \n",
      "iihta il e\n",
      " Batch index: 50, train loss: 3.2679190635681152, val_loss: 3.30224347114563, generated text\n",
      " \n",
      "s co p mve\n",
      " Batch index: 60, train loss: 3.2419238090515137, val_loss: 3.3058948516845703, generated text\n",
      " \n",
      " l: .ea:sM\n",
      " Batch index: 70, train loss: 3.230442523956299, val_loss: 3.2872843742370605, generated text\n",
      " \n",
      "tohid s d \n",
      " Batch index: 80, train loss: 3.1901049613952637, val_loss: 3.227417469024658, generated text\n",
      " \n",
      "coa,yosoOe\n",
      " Batch index: 90, train loss: 3.1123595237731934, val_loss: 3.2420284748077393, generated text\n",
      " \n",
      "r R a\n",
      "cie \n",
      " Batch index: 100, train loss: 3.1530251502990723, val_loss: 3.156932830810547, generated text\n",
      " \n",
      "no;uototfI\n",
      " Batch index: 110, train loss: 3.088366985321045, val_loss: 3.128541946411133, generated text\n",
      " \n",
      "A:a ,d n n\n",
      " Batch index: 120, train loss: 3.082252264022827, val_loss: 3.084439516067505, generated text\n",
      " \n",
      "ll ''omAnu\n",
      " Batch index: 130, train loss: 3.0683858394622803, val_loss: 3.1073527336120605, generated text\n",
      " \n",
      "ooaRthp w,\n",
      " Batch index: 140, train loss: 3.035062074661255, val_loss: 3.112946033477783, generated text\n",
      " \n",
      " \n",
      "n hiiR d\n",
      " Batch index: 150, train loss: 3.04952073097229, val_loss: 3.064709424972534, generated text\n",
      " \n",
      "fivevC ws\n",
      "\n",
      " Batch index: 160, train loss: 3.03106689453125, val_loss: 3.0399012565612793, generated text\n",
      " \n",
      "cahotRheYo\n",
      " Batch index: 170, train loss: 2.987414598464966, val_loss: 3.0909814834594727, generated text\n",
      " \n",
      "eoUlcebR d\n",
      " Batch index: 180, train loss: 3.0295958518981934, val_loss: 3.061502695083618, generated text\n",
      " \n",
      "i yariur o\n",
      " Batch index: 190, train loss: 2.986483573913574, val_loss: 3.0398006439208984, generated text\n",
      " \n",
      "ai te noo\n",
      "\n",
      " Batch index: 200, train loss: 3.0081467628479004, val_loss: 3.020596742630005, generated text\n",
      " \n",
      "ihi mot yc\n",
      " Batch index: 210, train loss: 2.9410061836242676, val_loss: 3.0344746112823486, generated text\n",
      " \n",
      "  tae vehe\n",
      " Batch index: 220, train loss: 3.014702796936035, val_loss: 3.0395426750183105, generated text\n",
      " \n",
      "rr: wikNc \n",
      " Batch index: 230, train loss: 2.9746899604797363, val_loss: 2.985379695892334, generated text\n",
      " \n",
      "ho mh, wh\n",
      "\n",
      " Batch index: 240, train loss: 3.011730670928955, val_loss: 3.012948989868164, generated text\n",
      " \n",
      "n   eeiscO\n",
      " Batch index: 250, train loss: 2.943504810333252, val_loss: 2.9899916648864746, generated text\n",
      " \n",
      "e: sa ehru\n",
      " Batch index: 260, train loss: 2.98551869392395, val_loss: 2.975982427597046, generated text\n",
      " \n",
      "ers\n",
      "nis,A \n",
      " Batch index: 270, train loss: 2.98779559135437, val_loss: 2.9867300987243652, generated text\n",
      " \n",
      "tehe cEeld\n",
      " Batch index: 280, train loss: 2.9686360359191895, val_loss: 2.986433506011963, generated text\n",
      " \n",
      "f n viw nt\n",
      " Batch index: 290, train loss: 2.9700469970703125, val_loss: 3.0082340240478516, generated text\n",
      " \n",
      "a!Odo$edhy\n",
      " Batch index: 300, train loss: 2.9638922214508057, val_loss: 2.946631908416748, generated text\n",
      " \n",
      "h,ruw od.N\n",
      " Batch index: 310, train loss: 2.9455368518829346, val_loss: 2.990050792694092, generated text\n",
      " \n",
      "egaroig\n",
      "am\n",
      " Batch index: 320, train loss: 2.9387049674987793, val_loss: 2.9796531200408936, generated text\n",
      " \n",
      " lo dy o f\n",
      " Batch index: 330, train loss: 2.95918607711792, val_loss: 2.9772233963012695, generated text\n",
      " \n",
      "eayhei\n",
      "at \n",
      " Batch index: 340, train loss: 2.9619877338409424, val_loss: 2.9445457458496094, generated text\n",
      " \n",
      "tipe rodob\n",
      " Batch index: 350, train loss: 2.9466404914855957, val_loss: 2.9702446460723877, generated text\n",
      " \n",
      "  ola tohe\n",
      " Batch index: 360, train loss: 2.9595088958740234, val_loss: 2.9906063079833984, generated text\n",
      " \n",
      " heh i,fda\n",
      " Batch index: 370, train loss: 2.957428216934204, val_loss: 2.9682669639587402, generated text\n",
      " \n",
      "ni oe Ii'e\n",
      " Batch index: 380, train loss: 2.9241204261779785, val_loss: 2.96287202835083, generated text\n",
      " \n",
      "Ei  redise\n",
      " Batch index: 390, train loss: 2.9437458515167236, val_loss: 2.9459660053253174, generated text\n",
      " \n",
      " e  Ii me\n",
      "\n",
      " Batch index: 400, train loss: 2.946988344192505, val_loss: 2.9523911476135254, generated text\n",
      " \n",
      "ehe' ,ho, \n",
      " Batch index: 410, train loss: 2.9444849491119385, val_loss: 2.993134021759033, generated text\n",
      " \n",
      "t, ratV ho\n",
      " Batch index: 420, train loss: 2.9243359565734863, val_loss: 2.9718055725097656, generated text\n",
      " \n",
      "hefod ov t\n",
      " Batch index: 430, train loss: 2.946500539779663, val_loss: 2.940359115600586, generated text\n",
      " \n",
      "tei Ri moc\n",
      " Batch index: 440, train loss: 2.9363739490509033, val_loss: 2.977870464324951, generated text\n",
      " \n",
      "iah lsr?Ie\n",
      " Batch index: 450, train loss: 2.958488702774048, val_loss: 2.962996482849121, generated text\n",
      " \n",
      " moT ldhew\n",
      " Batch index: 460, train loss: 2.9518227577209473, val_loss: 2.931766986846924, generated text\n",
      " \n",
      "ttesare;,d\n",
      " Batch index: 470, train loss: 2.9987025260925293, val_loss: 2.9977097511291504, generated text\n",
      " \n",
      "a hosm' l \n",
      " Batch index: 480, train loss: 2.905442714691162, val_loss: 2.9787464141845703, generated text\n",
      " \n",
      "  ue diia\n",
      "\n",
      " Batch index: 490, train loss: 2.9216952323913574, val_loss: 2.9797747135162354, generated text\n",
      " \n",
      "cmAou i wG\n",
      " Batch index: 500, train loss: 2.919389009475708, val_loss: 2.997016429901123, generated text\n",
      " \n",
      "e  taeli p\n",
      "Reached maximum number of matches. Training is now complete.\n"
     ]
    }
   ],
   "source": [
    "# Jupyter notebook for doing LoRA finetuning of GPT models.\n",
    "\n",
    "import os\n",
    "\n",
    "from datasets.text_dataset import TextDataset\n",
    "from tokenizers.char_tokenizer import CharTokenizer\n",
    "from layers.gpt_with_lora_finetuning import GPTWithLoRAFinetuning\n",
    "from layers import layer_utils\n",
    "from evaluate import Evaluator\n",
    "\n",
    "from collections.abc import Callable\n",
    "import torch, torch.nn as nn\n",
    "\n",
    "\n",
    "# Define hyperparameters.\n",
    "\n",
    "# Dataset hyper parameters\n",
    "data_filename = 'testdata/tinyshakespeare.txt'\n",
    "train_fraction = 0.9\n",
    "\n",
    "# Tokenizer hyperparameters.\n",
    "tokenizer = CharTokenizer(filename = data_filename)\n",
    "\n",
    "# Architecture hyperparameters.\n",
    "embedding_dimension = 64\n",
    "num_heads = 8\n",
    "head_dimension = 16\n",
    "num_decoder_blocks = 10\n",
    "\n",
    "# LoRA finetuning parameters.\n",
    "mode = 'finetuning'\n",
    "pretrained_model_path = 'output/gpt_pretrained_model.pt'\n",
    "\n",
    "# Training hyperparameters.\n",
    "max_block_size = 24\n",
    "batch_size = 32\n",
    "num_batches_to_train = 500\n",
    "\n",
    "# Evaluation hyperparameters.\n",
    "num_batches_to_evaluate = 10\n",
    "num_tokens_to_generate_during_evaluation = 10\n",
    "num_batches_between_evaluations = 10\n",
    "\n",
    "# Output parameters.\n",
    "output_model_path = 'output/gpt_lora_finetuned_model.pt'\n",
    "output_params_path = 'output/num_parameters_lora_finetuning.yaml'\n",
    "\n",
    "# Fixing seed for reproducing results.\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# Create directory corresponding to output_model_path if it does not exist.\n",
    "model_dirname = os.path.dirname(output_model_path)\n",
    "if not os.path.exists(model_dirname):\n",
    "    os.makedirs(model_dirname)\n",
    "\n",
    "# Generate train and validation datasets and data loaders.\n",
    "train_dataset = TextDataset(max_block_size, tokenizer, 'train', train_fraction, filename = data_filename)\n",
    "val_dataset = TextDataset(max_block_size, tokenizer, 'val', train_fraction, filename = data_filename)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size, shuffle = True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size, shuffle = True)\n",
    "\n",
    "# Define the model architecture and optimizer.\n",
    "model = GPTWithLoRAFinetuning(num_decoder_blocks, tokenizer.vocabulary_length(), embedding_dimension, num_heads, head_dimension, max_block_size, mode=mode, pretrained_model_path = pretrained_model_path)\n",
    "num_model_parameters = layer_utils.num_parameters(model, output_params_path)\n",
    "print(f'Number of parameters in the model is {num_model_parameters[\"total_trainable_parameters\"]}.')\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "evaluator = Evaluator()\n",
    "\n",
    "# Set device for training.\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Perform model training and evaluation.\n",
    "for (batch_index, train_batch) in enumerate(train_dataloader):\n",
    "    if batch_index > num_batches_to_train:\n",
    "        print('Reached maximum number of matches. Training is now complete.')\n",
    "        torch.save(model.state_dict(), output_model_path)\n",
    "        break\n",
    "\n",
    "    train_features = train_batch['features'].to(device)\n",
    "    train_labels = train_batch['labels'].to(device)\n",
    "    predictions, loss = model(train_features, train_labels)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if batch_index % num_batches_between_evaluations == 0:\n",
    "        (train_loss, val_loss) = evaluator.evaluate_train_and_validation_loss(train_dataloader, val_dataloader, model, num_batches_to_evaluate, device)\n",
    "        generated_text = evaluator.generate_text(model, num_tokens_to_generate_during_evaluation, tokenizer, device)\n",
    "        print(f' Batch index: {batch_index}, train loss: {train_loss}, val_loss: {val_loss}, generated text\\n {generated_text}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68db28c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6cebca37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in the model is 509121.\n",
      "torch.Size([128, 64])\n",
      "tensor([[ 0.0186,  0.0146],\n",
      "        [-0.0701,  0.0618]])\n"
     ]
    }
   ],
   "source": [
    "from layers.gpt import GPT\n",
    "model = GPT(num_decoder_blocks, tokenizer.vocabulary_length(), embedding_dimension, num_heads, head_dimension, max_block_size)\n",
    "model.load_state_dict(torch.load(pretrained_model_path))\n",
    "num_model_parameters = layer_utils.num_parameters(model, output_params_path)\n",
    "print(f'Number of parameters in the model is {num_model_parameters[\"total_trainable_parameters\"]}.')\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    #print(name, param.data)\n",
    "    \n",
    "    if name == 'transformer_decoders.0.attention_layer.Wq.weight':\n",
    "        print(param.data.shape)\n",
    "        print(param.data[1:3, 1:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d40b1cff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.named_modules at 0x7fcf4b85e6d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d86175d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d15c39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
